import streamlit as st
import plotly.graph_objects as go
import torch
import math
import os
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, GPT2LMHeadModel, pipeline
from datasets import load_dataset
from rouge_score import rouge_scorer

# === G√©n√©rateur de texte r√©el ===
@st.cache_resource(show_spinner=False)
def get_generator():
    return pipeline("text-generation", model="distilgpt2")

def generate_text(prompt):
    generator = get_generator()
    output = generator(prompt, max_length=50, do_sample=True, top_k=30)[0]["generated_text"]
    return output

# === R√©sum√© avec facebook/bart-base ===
@st.cache_resource(show_spinner=False)
def get_summarizer():
    return pipeline("summarization", model="facebook/bart-base")

def summarize_text(text):
    summarizer = get_summarizer()
    summary = summarizer(text, max_length=50, min_length=20, do_sample=False)[0]['summary_text']
    return summary

@st.cache_resource(show_spinner=False)
def get_toxic_classifier():
    return pipeline("text-classification", model="unitary/toxic-bert", return_all_scores=True)

def automatic_filter(text):
    classifier = get_toxic_classifier()
    result = classifier(text)[0]
    toxic_labels = [label['label'] for label in result if label['score'] > 0.6 and label['label'].lower() != 'non-toxic']
    return len(toxic_labels) == 0, toxic_labels

def check_similarity(prompt, summary):
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    score = scorer.score(prompt, summary)
    rouge_score = score["rougeL"].fmeasure
    return rouge_score > 0.3, rouge_score

def compute_perplexity(texts):
    tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
    model = GPT2LMHeadModel.from_pretrained("distilgpt2")
    model.eval()
    total_log_likelihood = 0
    total_length = 0
    for text in texts:
        encodings = tokenizer(text, return_tensors="pt")
        input_ids = encodings.input_ids
        with torch.no_grad():
            outputs = model(input_ids, labels=input_ids)
            log_likelihood = outputs.loss.item() * input_ids.size(1)
        total_log_likelihood += log_likelihood
        total_length += input_ids.size(1)
    perplexity = math.exp(total_log_likelihood / total_length)
    return perplexity


import gspread
from google.oauth2.service_account import Credentials

# Connexion Google Sheets
GSHEET_ID = "1DjsS_4t8rL5H_bSMtDX1MhzSJhimIjzBV1nLnKy881M"
GSHEET_CREDS = Credentials.from_service_account_info(st.secrets["gcp_service_account"])
gc = gspread.authorize(GSHEET_CREDS)
sheet = gc.open_by_key(GSHEET_ID).sheet1

def load_prompts():
    try:
        records = sheet.col_values(1)[1:]  # skip header
        return list(reversed(records[-10:]))
    except Exception as e:
        return []

def save_prompt(prompt):
    try:
        sheet.append_row([prompt])
    except Exception as e:
        pass

# === Interface Streamlit ===
st.title("üß† G√©n√©rateur de Texte + Filtrage √âthique")

if "stats" not in st.session_state:
    st.session_state.stats = {
        "total": 0,
        "passed": 0,
        "rejected": 0,
        "similarities": [],
        "causes": {}
    }

prompt = st.text_input("Entrez un prompt :", value="This movie was")

if st.button("G√©n√©rer"):
    with st.spinner("G√©n√©ration en cours..."):
        generated = generate_text(prompt)
        summary = summarize_text(generated)
        is_relevant, sim_score = check_similarity(prompt, summary)

        passes_filters, reasons = automatic_filter(generated)

        st.session_state.stats["total"] += 1
        st.session_state.stats["passed"] += int(passes_filters)
        st.session_state.stats["rejected"] += int(not passes_filters)
        st.session_state.stats["similarities"].append(sim_score)

    st.subheader("Texte g√©n√©r√©")
    st.write(generated)

    st.subheader("R√©sum√©")
    st.write(summary)

    st.subheader("Qualit√© & Filtres")
    st.write(f"üîé Similarit√© prompt/r√©sum√© : {sim_score:.2f} ({'OK' if is_relevant else 'BAS'})")
    st.write(f"üö°Ô∏è Filtrage √©thique : {'‚úÖ Accept√©' if passes_filters else '‚ùå Rejet√©'}")
    if not passes_filters:
        st.write("**Raisons du rejet :**")
        for r in reasons:
            st.write(f"- {r}")

    with st.spinner("Calcul de perplexit√©..."):
        try:
            ppl = compute_perplexity([generated])
            st.write(f"üßπ Perplexit√© du texte g√©n√©r√© : {ppl:.2f}")
        except Exception as e:
            st.warning(f"Erreur perplexit√© : {e}")

# === Dashboard temps r√©el ===
st.sidebar.header("üìä Statistiques en temps r√©el")
st.sidebar.write(f"Total g√©n√©r√©s : {st.session_state.stats['total']}")
st.sidebar.write(f"‚úÖ Accept√©s : {st.session_state.stats['passed']}")
st.sidebar.write(f"‚ùå Rejet√©s : {st.session_state.stats['rejected']}")

if st.session_state.stats['similarities']:
    fig = go.Figure()
    fig.add_trace(go.Scatter(y=st.session_state.stats['similarities'], mode='lines+markers', name='Similarit√©'))
    fig.update_layout(title='√âvolution de la similarit√© (prompt vs r√©sum√©)', yaxis=dict(range=[0, 1]))
    st.sidebar.plotly_chart(fig, use_container_width=True)

# === Fine-tuning d√©sactiv√© temporairement ===
# from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification

# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = torch.argmax(torch.tensor(logits), dim=1)
#     acc = accuracy_score(labels, predictions)
#     return {"accuracy": acc}

# @st.cache_resource
# def fine_tune_on_imdb():
#     imdb = load_dataset("imdb", split="train[:1%]").train_test_split(test_size=0.2)
#     tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
#     model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

#     def preprocess(example):
#         return tokenizer(example["text"], padding="max_length", truncation=True)

#     tokenized = imdb.map(preprocess, batched=True)

#     args = TrainingArguments(
#         output_dir="results",
#         evaluation_strategy="epoch",
#         per_device_train_batch_size=8,
#         per_device_eval_batch_size=8,
#         num_train_epochs=1,
#         logging_dir="logs",
#         disable_tqdm=True
#     )

#     trainer = Trainer(
#         model=model,
#         args=args,
#         train_dataset=tokenized["train"],
#         eval_dataset=tokenized["test"],
#         compute_metrics=compute_metrics
#     )

#     trainer.train()
#     metrics = trainer.evaluate()
#     return metrics

# with st.expander("üîß Fine-tuning personnalis√© (IMDb)"):
#     if st.button("Lancer le fine-tuning"):
#         with st.spinner("Entra√Ænement en cours (1% IMDb)..."):
#             results = fine_tune_on_imdb()
#         st.success("Fine-tuning termin√© ‚úÖ")
#         st.write(f"üéØ Accuracy: {results['eval_accuracy']:.2f}")
#         st.write(f"üìâ Loss: {results['eval_loss']:.4f}")

